{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Train/Test/Val Split Analysis\n\nThis notebook analyzes the final train/validation/test split data for the energy forecasting models.\n\n## Configuration\n- **Dataset**: `building` (TrainDatasetBuilding)\n- **Split Method**: `time` (time-based split, 80/10/10 ratio)\n- **Resolutions**: Daily and Hourly\n\n## Overview\n\nThe train/test/val split is performed in `src/energy_forecast/utils/train_test_val_split.py`:\n- **Time-based split**: Each building's time series is split chronologically (80% train, 10% val, 10% test)\n- Series too short to create at least one training example (< lag_in + lag_out) are discarded\n- Split preserves temporal order within each building",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom loguru import logger\n\nfrom src.energy_forecast.dataset import TrainDatasetBuilding\nfrom src.energy_forecast.utils.train_test_val_split import get_train_test_val_split\n\n# Set display options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:43:33.799852874Z",
     "start_time": "2025-12-31T11:43:31.600256470Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-12-31 12:43:31.619\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m15\u001B[0m - \u001B[1mPROJ_ROOT path is: /home/marja/PycharmProjects/energy-forecast-wahl\u001B[0m\n",
      "2025-12-31 12:43:32.208738: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-31 12:43:32.219198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767181412.230712   26127 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767181412.234831   26127 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767181412.243715   26127 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767181412.243729   26127 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767181412.243731   26127 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767181412.243732   26127 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-31 12:43:32.246829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## Daily Dataset (7-day forecast)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Configuration for daily dataset\nconfig_daily = {\n    \"dataset\": \"building\",\n    \"res\": \"daily\",\n    \"interpolate\": 1,\n    \"lag_in\": 7,   # 7 days of historical data\n    \"lag_out\": 7,  # 7 days forecast\n    \"n_in\": 7,\n    \"n_out\": 7,\n    \"energy\": \"all\",\n    \"train_test_split_method\": \"time\",\n    \"scale_mode\": \"individual\",\n    \"scaler\": \"standard\",\n    \"feature_code\": 10  # FEATURE_SET_10 for building features\n}\n\nprint(\"Loading daily dataset...\")\nds_daily = TrainDatasetBuilding(config_daily)\nds_daily.load_feat_data(interpolate=True)\nds_daily.preprocess()\n\nprint(f\"\\nDataset shape before split: {ds_daily.df.shape}\")\nprint(f\"Number of unique buildings: {ds_daily.df['id'].n_unique()}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:43:33.957013397Z",
     "start_time": "2025-12-31T11:43:33.801492684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading daily dataset...\n",
      "\u001B[32m2025-12-31 12:43:33.936\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.dataset\u001B[0m:\u001B[36mpreprocess\u001B[0m:\u001B[36m606\u001B[0m - \u001B[1mTraining Features: ['heated_area', 'wpgt', 'weekend', 'tsun', 'holiday', 'pres', 'tavg', 'daily_avg', 'wspd', 'tmax', 'tmin', 'hum_avg', 'hum_max', 'typ_0', 'primary_energy_gas', 'typ_1', 'primary_energy_district heating', 'wdir', 'hum_min', 'typ_4', 'diff', 'typ_2', 'prcp', 'day_of_month_sin', 'weekday_sin', 'day_of_month_cos', 'weekday_cos']\u001B[0m\n",
      "\n",
      "Dataset shape before split: (56955, 48)\n",
      "Number of unique buildings: 132\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# Perform train/test/val split\nprint(\"Performing train/test/val split...\")\nds_daily = get_train_test_val_split(ds_daily)\n\nprint(f\"\\nSplit completed!\")\nprint(f\"Number of discarded series (too short): {len(ds_daily.discarded_ids)}\")\nprint(f\"Remaining series: {ds_daily.df['id'].n_unique() - len(ds_daily.discarded_ids)}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:43:34.135404393Z",
     "start_time": "2025-12-31T11:43:33.963271516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing train/test/val split...\n",
      "\u001B[32m2025-12-31 12:43:34.089\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mtrain_test_split_time_based\u001B[0m:\u001B[36m85\u001B[0m - \u001B[1mRemoved 40 series because they were too short\u001B[0m\n",
      "\u001B[32m2025-12-31 12:43:34.089\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mtrain_test_split_time_based\u001B[0m:\u001B[36m87\u001B[0m - \u001B[1mRemaining series: 92\u001B[0m\n",
      "\u001B[32m2025-12-31 12:43:34.129\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m236\u001B[0m - \u001B[1mTrain data shape: (43343, 26)\u001B[0m\n",
      "\u001B[32m2025-12-31 12:43:34.129\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m237\u001B[0m - \u001B[1mTest data shape: (5402, 26)\u001B[0m\n",
      "\u001B[32m2025-12-31 12:43:34.129\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m238\u001B[0m - \u001B[1mValidation data shape: (5361, 26)\u001B[0m\n",
      "\u001B[32m2025-12-31 12:43:34.129\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m239\u001B[0m - \u001B[1mTraining on 27 features\u001B[0m\n",
      "\n",
      "Split completed!\n",
      "Number of discarded series (too short): 40\n",
      "Remaining series: 92\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "### Date Ranges Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze date ranges for train/val/test splits\nimport polars as pl\n\n# Get dataframe with datetime\ndf_daily = ds_daily.df.filter(~pl.col(\"id\").is_in(ds_daily.discarded_ids))\n\n# Get train/val/test data using indices\ntrain_df = df_daily.filter(pl.col(\"index\").is_in(ds_daily.train_idxs))\nval_df = df_daily.filter(pl.col(\"index\").is_in(ds_daily.val_idxs))\ntest_df = df_daily.filter(pl.col(\"index\").is_in(ds_daily.test_idxs))\n\n# Compute date ranges\ndate_ranges = pd.DataFrame({\n    'Split': ['Train', 'Validation', 'Test'],\n    'Start Date': [\n        train_df['datetime'].min(),\n        val_df['datetime'].min(),\n        test_df['datetime'].min()\n    ],\n    'End Date': [\n        train_df['datetime'].max(),\n        val_df['datetime'].max(),\n        test_df['datetime'].max()\n    ],\n    'Total Days': [\n        (train_df['datetime'].max() - train_df['datetime'].min()).days,\n        (val_df['datetime'].max() - val_df['datetime'].min()).days,\n        (test_df['datetime'].max() - test_df['datetime'].min()).days\n    ],\n    'Total Samples': [len(train_df), len(val_df), len(test_df)]\n})\n\nprint(\"Daily Dataset - Date Ranges per Split:\\n\")\nprint(date_ranges.to_string(index=False))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:43:34.209658084Z",
     "start_time": "2025-12-31T11:43:34.136619711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Dataset - Date Ranges per Split:\n",
      "\n",
      "     Split Start Date   End Date  Total Days  Total Samples\n",
      "     Train 2017-03-21 2023-08-01        2324          43343\n",
      "Validation 2017-11-08 2023-09-19        2141           5361\n",
      "      Test 2017-10-13 2023-08-19        2136           5402\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "### Per-Series Statistics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Count unique series per split\ntrain_series = train_df.select('id').unique()\nval_series = val_df.select('id').unique()\ntest_series = test_df.select('id').unique()\n\nprint(f\"Unique series per split:\")\nprint(f\"  Train: {len(train_series)} series\")\nprint(f\"  Validation: {len(val_series)} series\")\nprint(f\"  Test: {len(test_series)} series\")\nprint(f\"  Total: {len(train_series)} series (same buildings across all splits)\")\n\n# Compute statistics per series for all splits\ntrain_stats = train_df.group_by('id').agg([\n    pl.len().alias('count'),\n    pl.col('datetime').min().alias('start'),\n    pl.col('datetime').max().alias('end')\n])\n\nval_stats = val_df.group_by('id').agg([\n    pl.len().alias('count'),\n    pl.col('datetime').min().alias('start'),\n    pl.col('datetime').max().alias('end')\n])\n\ntest_stats = test_df.group_by('id').agg([\n    pl.len().alias('count'),\n    pl.col('datetime').min().alias('start'),\n    pl.col('datetime').max().alias('end')\n])\n\n# Display statistics\nstats_summary = pd.DataFrame({\n    'Split': ['Train', 'Validation', 'Test'],\n    'Min': [train_stats['count'].min(), val_stats['count'].min(), test_stats['count'].min()],\n    'Max': [train_stats['count'].max(), val_stats['count'].max(), test_stats['count'].max()],\n    'Mean': [train_stats['count'].mean(), val_stats['count'].mean(), test_stats['count'].mean()],\n    'Median': [train_stats['count'].median(), val_stats['count'].median(), test_stats['count'].median()]\n})\n\nprint(f\"\\nPer-series sample statistics:\")\nprint(stats_summary.to_string(index=False))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:49:16.749561533Z",
     "start_time": "2025-12-31T11:49:16.659658105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique series per split:\n",
      "  Train: 92 series\n",
      "  Validation: 92 series\n",
      "  Test: 92 series\n",
      "  Total: 92 series (same buildings across all splits)\n",
      "\n",
      "Per-series sample statistics:\n",
      "     Split  Min  Max       Mean  Median\n",
      "     Train  124 1333 471.119565   384.5\n",
      "Validation   15  166  58.271739    47.5\n",
      "      Test   15  166  58.717391    48.0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset Reduction Analysis\n\nThe data goes through several preprocessing steps that reduce the total samples:\n1. **Original interpolated data**: Raw dataset with all buildings and timestamps\n2. **Lag feature creation**: Adding `lag_in` historical and `lag_out` future columns creates NaN values at the start/end of each series\n3. **Null removal**: Rows with NaN lag features are dropped\n4. **Short series filtering**: Buildings with insufficient data after split are discarded\n\nLet's compare the original dataset size to the final preprocessed size.",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:57:55.722512300Z",
     "start_time": "2025-12-31T11:57:55.651388654Z"
    }
   },
   "cell_type": "code",
   "source": "# Load original interpolated dataset (before preprocessing)\nimport polars as pl\noriginal_daily = pl.read_csv('../data/processed/dataset_interpolate_daily_feat.csv')\n\nprint(\"Dataset Size Comparison (Daily):\\n\")\nprint(f\"Original interpolated dataset: {len(original_daily):,} samples\")\nprint(f\"After preprocessing (with lag features): {len(ds_daily.df):,} samples\")\nprint(f\"  Reduction: {len(original_daily) - len(ds_daily.df):,} samples ({((len(original_daily) - len(ds_daily.df)) / len(original_daily) * 100):.1f}%)\")\nprint(f\"\\nAfter discarding {len(ds_daily.discarded_ids)} short series: {len(df_daily):,} samples\")\nprint(f\"  Total reduction: {len(original_daily) - len(df_daily):,} samples ({((len(original_daily) - len(df_daily)) / len(original_daily) * 100):.1f}%)\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"Final Windowed Samples per Split:\")\nprint(f\"{'='*70}\")\nprint(f\"Train samples: {ds_daily.X_train.shape[0]:,} samples × {ds_daily.X_train.shape[1]} features\")\nprint(f\"Validation samples: {ds_daily.X_val.shape[0]:,} samples × {ds_daily.X_val.shape[1]} features\")\nprint(f\"Test samples: {ds_daily.X_test.shape[0]:,} samples × {ds_daily.X_test.shape[1]} features\")\nprint(f\"\\nTarget shape:\")\nprint(f\"Train targets: {ds_daily.y_train.shape[0]:,} samples × {ds_daily.y_train.shape[1]} timesteps\")\nprint(f\"Validation targets: {ds_daily.y_val.shape[0]:,} samples × {ds_daily.y_val.shape[1]} timesteps\")\nprint(f\"Test targets: {ds_daily.y_test.shape[0]:,} samples × {ds_daily.y_test.shape[1]} timesteps\")\n\n# Calculate per-split percentages\ntotal_final = len(ds_daily.X_train) + len(ds_daily.X_val) + len(ds_daily.X_test)\nprint(f\"\\nSplit distribution:\")\nprint(f\"  Train: {len(ds_daily.X_train)/total_final*100:.1f}%\")\nprint(f\"  Validation: {len(ds_daily.X_val)/total_final*100:.1f}%\")\nprint(f\"  Test: {len(ds_daily.X_test)/total_final*100:.1f}%\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size Comparison (Daily):\n",
      "\n",
      "Original interpolated dataset: 103,999 samples\n",
      "After preprocessing (with lag features): 56,955 samples\n",
      "  Reduction: 47,044 samples (45.2%)\n",
      "\n",
      "After discarding 40 short series: 54,106 samples\n",
      "  Total reduction: 49,893 samples (48.0%)\n",
      "\n",
      "======================================================================\n",
      "Final Windowed Samples per Split:\n",
      "======================================================================\n",
      "Train samples: 43,343 samples × 26 features\n",
      "Validation samples: 5,361 samples × 26 features\n",
      "Test samples: 5,402 samples × 26 features\n",
      "\n",
      "Target shape:\n",
      "Train targets: 43,343 samples × 7 timesteps\n",
      "Validation targets: 5,361 samples × 7 timesteps\n",
      "Test targets: 5,402 samples × 7 timesteps\n",
      "\n",
      "Split distribution:\n",
      "  Train: 80.1%\n",
      "  Validation: 9.9%\n",
      "  Test: 10.0%\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "## Hourly Dataset (72-hour forecast)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Configuration for hourly dataset\nconfig_hourly = {\n    \"dataset\": \"building\",\n    \"res\": \"hourly\",\n    \"interpolate\": 1,\n    \"lag_in\": 72,   # 72 hours (3 days) of historical data\n    \"lag_out\": 72,  # 72 hours (3 days) forecast\n    \"n_in\": 72,\n    \"n_out\": 72,\n    \"energy\": \"all\",\n    \"train_test_split_method\": \"time\",\n    \"scale_mode\": \"individual\",\n    \"scaler\": \"standard\",\n    \"feature_code\": 15  # FEATURE_SET_15 for hourly building features\n}\n\nprint(\"Loading hourly dataset...\")\nds_hourly = TrainDatasetBuilding(config_hourly)\nds_hourly.load_feat_data(interpolate=True)\nds_hourly.preprocess()\n\nprint(f\"\\nDataset shape before split: {ds_hourly.df.shape}\")\nprint(f\"Number of unique buildings: {ds_hourly.df['id'].n_unique()}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:59:03.023836206Z",
     "start_time": "2025-12-31T11:58:59.893089050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hourly dataset...\n",
      "\u001B[32m2025-12-31 12:59:02.977\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.dataset\u001B[0m:\u001B[36mpreprocess\u001B[0m:\u001B[36m606\u001B[0m - \u001B[1mTraining Features: ['heated_area', 'wpgt', 'coco', 'weekend', 'tsun', 'holiday', 'pres', 'prcp', 'daily_avg', 'wspd', 'temp', 'dwpt', 'typ_0', 'primary_energy_gas', 'rhum', 'typ_1', 'primary_energy_district heating', 'wdir', 'typ_4', 'diff', 'typ_2', 'snow', 'day_of_month_sin', 'weekday_sin', 'day_of_month_cos', 'weekday_cos']\u001B[0m\n",
      "\n",
      "Dataset shape before split: (142085, 176)\n",
      "Number of unique buildings: 44\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "# Perform train/test/val split\nprint(\"Performing train/test/val split...\")\nds_hourly = get_train_test_val_split(ds_hourly)\n\nprint(f\"\\nSplit completed!\")\nprint(f\"Number of discarded series (too short): {len(ds_hourly.discarded_ids)}\")\nprint(f\"Remaining series: {ds_hourly.df['id'].n_unique() - len(ds_hourly.discarded_ids)}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:59:07.440023822Z",
     "start_time": "2025-12-31T11:59:07.169525310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing train/test/val split...\n",
      "\u001B[32m2025-12-31 12:59:07.283\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mtrain_test_split_time_based\u001B[0m:\u001B[36m85\u001B[0m - \u001B[1mRemoved 19 series because they were too short\u001B[0m\n",
      "\u001B[32m2025-12-31 12:59:07.283\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mtrain_test_split_time_based\u001B[0m:\u001B[36m87\u001B[0m - \u001B[1mRemaining series: 25\u001B[0m\n",
      "\u001B[32m2025-12-31 12:59:07.430\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m236\u001B[0m - \u001B[1mTrain data shape: (108849, 25)\u001B[0m\n",
      "\u001B[32m2025-12-31 12:59:07.430\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m237\u001B[0m - \u001B[1mTest data shape: (13603, 25)\u001B[0m\n",
      "\u001B[32m2025-12-31 12:59:07.430\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m238\u001B[0m - \u001B[1mValidation data shape: (13591, 25)\u001B[0m\n",
      "\u001B[32m2025-12-31 12:59:07.431\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.energy_forecast.utils.train_test_val_split\u001B[0m:\u001B[36mget_train_test_val_split\u001B[0m:\u001B[36m239\u001B[0m - \u001B[1mTraining on 26 features\u001B[0m\n",
      "\n",
      "Split completed!\n",
      "Number of discarded series (too short): 19\n",
      "Remaining series: 25\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": "### Date Ranges Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze date ranges for train/val/test splits (hourly)\ndf_hourly = ds_hourly.df.filter(~pl.col(\"id\").is_in(ds_hourly.discarded_ids))\n\n# Get train/val/test data using indices\ntrain_df_h = df_hourly.filter(pl.col(\"index\").is_in(ds_hourly.train_idxs))\nval_df_h = df_hourly.filter(pl.col(\"index\").is_in(ds_hourly.val_idxs))\ntest_df_h = df_hourly.filter(pl.col(\"index\").is_in(ds_hourly.test_idxs))\n\n# Compute date ranges\ndate_ranges_h = pd.DataFrame({\n    'Split': ['Train', 'Validation', 'Test'],\n    'Start Date': [\n        train_df_h['datetime'].min(),\n        val_df_h['datetime'].min(),\n        test_df_h['datetime'].min()\n    ],\n    'End Date': [\n        train_df_h['datetime'].max(),\n        val_df_h['datetime'].max(),\n        test_df_h['datetime'].max()\n    ],\n    'Total Days': [\n        (train_df_h['datetime'].max() - train_df_h['datetime'].min()).days,\n        (val_df_h['datetime'].max() - val_df_h['datetime'].min()).days,\n        (test_df_h['datetime'].max() - test_df_h['datetime'].min()).days\n    ],\n    'Total Hours': [\n        int((train_df_h['datetime'].max() - train_df_h['datetime'].min()).total_seconds() / 3600),\n        int((val_df_h['datetime'].max() - val_df_h['datetime'].min()).total_seconds() / 3600),\n        int((test_df_h['datetime'].max() - test_df_h['datetime'].min()).total_seconds() / 3600)\n    ],\n    'Total Samples': [len(train_df_h), len(val_df_h), len(test_df_h)]\n})\n\nprint(\"Hourly Dataset - Date Ranges per Split:\\n\")\nprint(date_ranges_h.to_string(index=False))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:59:12.312096316Z",
     "start_time": "2025-12-31T11:59:12.240436452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly Dataset - Date Ranges per Split:\n",
      "\n",
      "     Split          Start Date            End Date  Total Days  Total Hours  Total Samples\n",
      "     Train 2021-10-01 04:00:00 2023-08-29 16:00:00         697        16740         108849\n",
      "Validation 2021-12-21 05:00:00 2023-09-22 08:00:00         640        15363          13591\n",
      "      Test 2021-12-12 05:00:00 2023-09-14 22:00:00         641        15401          13603\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": "### Per-Series Statistics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Count unique series per split\ntrain_series_h = train_df_h.select('id').unique()\nval_series_h = val_df_h.select('id').unique()\ntest_series_h = test_df_h.select('id').unique()\n\nprint(f\"Unique series per split:\")\nprint(f\"  Train: {len(train_series_h)} series\")\nprint(f\"  Validation: {len(val_series_h)} series\")\nprint(f\"  Test: {len(test_series_h)} series\")\nprint(f\"  Total: {len(train_series_h)} series (same buildings across all splits)\")\n\n# Compute statistics per series for all splits\ntrain_stats_h = train_df_h.group_by('id').agg([\n    pl.len().alias('count'),\n    pl.col('datetime').min().alias('start'),\n    pl.col('datetime').max().alias('end')\n])\n\nval_stats_h = val_df_h.group_by('id').agg([\n    pl.len().alias('count'),\n    pl.col('datetime').min().alias('start'),\n    pl.col('datetime').max().alias('end')\n])\n\ntest_stats_h = test_df_h.group_by('id').agg([\n    pl.len().alias('count'),\n    pl.col('datetime').min().alias('start'),\n    pl.col('datetime').max().alias('end')\n])\n\n# Display statistics\nstats_summary_h = pd.DataFrame({\n    'Split': ['Train', 'Validation', 'Test'],\n    'Min': [train_stats_h['count'].min(), val_stats_h['count'].min(), test_stats_h['count'].min()],\n    'Max': [train_stats_h['count'].max(), val_stats_h['count'].max(), test_stats_h['count'].max()],\n    'Mean': [train_stats_h['count'].mean(), val_stats_h['count'].mean(), test_stats_h['count'].mean()],\n    'Median': [train_stats_h['count'].median(), val_stats_h['count'].median(), test_stats_h['count'].median()]\n})\n\nprint(f\"\\nPer-series sample statistics:\")\nprint(stats_summary_h.to_string(index=False))",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:59:15.602363063Z",
     "start_time": "2025-12-31T11:59:15.552077922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique series per split:\n",
      "  Train: 25 series\n",
      "  Validation: 25 series\n",
      "  Test: 25 series\n",
      "  Total: 25 series (same buildings across all splits)\n",
      "\n",
      "Per-series sample statistics:\n",
      "     Split  Min   Max    Mean  Median\n",
      "     Train 1431 11898 4353.96  2732.0\n",
      "Validation  178  1487  543.64   341.0\n",
      "      Test  179  1487  544.12   341.0\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset Reduction Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load original interpolated dataset (before preprocessing)\noriginal_hourly = pl.read_csv('../data/processed/dataset_interpolate_hourly_feat.csv')\n\nprint(\"Dataset Size Comparison (Hourly):\\n\")\nprint(f\"Original interpolated dataset: {len(original_hourly):,} samples\")\nprint(f\"After preprocessing (with lag features): {len(ds_hourly.df):,} samples\")\nprint(f\"  Reduction: {len(original_hourly) - len(ds_hourly.df):,} samples ({((len(original_hourly) - len(ds_hourly.df)) / len(original_hourly) * 100):.1f}%)\")\nprint(f\"\\nAfter discarding {len(ds_hourly.discarded_ids)} short series: {len(df_hourly):,} samples\")\nprint(f\"  Total reduction: {len(original_hourly) - len(df_hourly):,} samples ({((len(original_hourly) - len(df_hourly)) / len(original_hourly) * 100):.1f}%)\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"Final Windowed Samples per Split:\")\nprint(f\"{'='*70}\")\nprint(f\"Train samples: {ds_hourly.X_train.shape[0]:,} samples × {ds_hourly.X_train.shape[1]} features\")\nprint(f\"Validation samples: {ds_hourly.X_val.shape[0]:,} samples × {ds_hourly.X_val.shape[1]} features\")\nprint(f\"Test samples: {ds_hourly.X_test.shape[0]:,} samples × {ds_hourly.X_test.shape[1]} features\")\nprint(f\"\\nTarget shape:\")\nprint(f\"Train targets: {ds_hourly.y_train.shape[0]:,} samples × {ds_hourly.y_train.shape[1]} timesteps\")\nprint(f\"Validation targets: {ds_hourly.y_val.shape[0]:,} samples × {ds_hourly.y_val.shape[1]} timesteps\")\nprint(f\"Test targets: {ds_hourly.y_test.shape[0]:,} samples × {ds_hourly.y_test.shape[1]} timesteps\")\n\n# Calculate per-split percentages\ntotal_final_h = len(ds_hourly.X_train) + len(ds_hourly.X_val) + len(ds_hourly.X_test)\nprint(f\"\\nSplit distribution:\")\nprint(f\"  Train: {len(ds_hourly.X_train)/total_final_h*100:.1f}%\")\nprint(f\"  Validation: {len(ds_hourly.X_val)/total_final_h*100:.1f}%\")\nprint(f\"  Test: {len(ds_hourly.X_test)/total_final_h*100:.1f}%\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T11:59:18.762240623Z",
     "start_time": "2025-12-31T11:59:18.606997228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size Comparison (Hourly):\n",
      "\n",
      "Original interpolated dataset: 834,313 samples\n",
      "After preprocessing (with lag features): 142,085 samples\n",
      "  Reduction: 692,228 samples (83.0%)\n",
      "\n",
      "After discarding 19 short series: 136,043 samples\n",
      "  Total reduction: 698,270 samples (83.7%)\n",
      "\n",
      "======================================================================\n",
      "Final Windowed Samples per Split:\n",
      "======================================================================\n",
      "Train samples: 108,849 samples × 25 features\n",
      "Validation samples: 13,591 samples × 25 features\n",
      "Test samples: 13,603 samples × 25 features\n",
      "\n",
      "Target shape:\n",
      "Train targets: 108,849 samples × 72 timesteps\n",
      "Validation targets: 13,591 samples × 72 timesteps\n",
      "Test targets: 13,603 samples × 72 timesteps\n",
      "\n",
      "Split distribution:\n",
      "  Train: 80.0%\n",
      "  Validation: 10.0%\n",
      "  Test: 10.0%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "## Summary Comparison\n\nKey insights from the train/test/val split analysis:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Summary comparison\n# Calculate total reduction from original interpolated datasets\ntotal_final_daily = len(ds_daily.X_train) + len(ds_daily.X_val) + len(ds_daily.X_test)\ntotal_final_hourly = len(ds_hourly.X_train) + len(ds_hourly.X_val) + len(ds_hourly.X_test)\n\nreduction_daily = ((len(original_daily) - total_final_daily) / len(original_daily) * 100)\nreduction_hourly = ((len(original_hourly) - total_final_hourly) / len(original_hourly) * 100)\n\nsummary = pd.DataFrame({\n    'Metric': [\n        'Forecast Horizon',\n        'Lag In (History)',\n        'Lag Out (Future)',\n        'Remaining Series',\n        'Discarded Series',\n        'Train Samples (Windowed)',\n        'Val Samples (Windowed)',\n        'Test Samples (Windowed)',\n        'Total Features',\n        'Total Reduction (%)'\n    ],\n    'Daily': [\n        '7 days',\n        '7 days',\n        '7 days',\n        len(train_series),\n        len(ds_daily.discarded_ids),\n        f\"{len(ds_daily.X_train):,}\",\n        f\"{len(ds_daily.X_val):,}\",\n        f\"{len(ds_daily.X_test):,}\",\n        ds_daily.X_train.shape[1],\n        f\"{reduction_daily:.1f}%\"\n    ],\n    'Hourly': [\n        '72 hours (3 days)',\n        '72 hours (3 days)',\n        '72 hours (3 days)',\n        len(train_series_h),\n        len(ds_hourly.discarded_ids),\n        f\"{len(ds_hourly.X_train):,}\",\n        f\"{len(ds_hourly.X_val):,}\",\n        f\"{len(ds_hourly.X_test):,}\",\n        ds_hourly.X_train.shape[1],\n        f\"{reduction_hourly:.1f}%\"\n    ]\n})\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: Daily vs Hourly Dataset Comparison\")\nprint(\"=\"*80 + \"\\n\")\nprint(summary.to_string(index=False))\nprint(\"\\n\" + \"=\"*80)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T12:01:00.040522833Z",
     "start_time": "2025-12-31T12:00:59.953885669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY: Daily vs Hourly Dataset Comparison\n",
      "================================================================================\n",
      "\n",
      "                  Metric  Daily            Hourly\n",
      "        Forecast Horizon 7 days 72 hours (3 days)\n",
      "        Lag In (History) 7 days 72 hours (3 days)\n",
      "        Lag Out (Future) 7 days 72 hours (3 days)\n",
      "        Remaining Series     92                25\n",
      "        Discarded Series     40                19\n",
      "Train Samples (Windowed) 43,343           108,849\n",
      "  Val Samples (Windowed)  5,361            13,591\n",
      " Test Samples (Windowed)  5,402            13,603\n",
      "          Total Features     26                25\n",
      "     Total Reduction (%)  48.0%             83.7%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": "### Key Observations\n\n1. **Time-based Split**: The 80/10/10 split ensures temporal ordering is preserved within each building's time series, making it suitable for time series forecasting evaluation.\n\n2. **Sliding Window Effect**: Converting raw time series to sliding window format reduces the number of samples:\n   - Each window needs `lag_in` + `lag_out` consecutive timesteps\n   - First `lag_in` and last `lag_out-1` samples cannot form complete windows\n   - Larger windows (72 hours for hourly) cause more reduction than smaller windows (7 days for daily)\n\n3. **Series Filtering**: Buildings with insufficient data (< `lag_in` + `lag_out` samples in any split) are discarded to ensure every building can contribute at least one training example.\n\n4. **Feature Dimensionality**: \n   - Daily dataset uses FEATURE_SET_10 (building metadata + daily weather)\n   - Hourly dataset uses FEATURE_SET_15 (building metadata + hourly weather)\n\n5. **Temporal Coverage**: Both datasets span similar time ranges but at different granularities, enabling both short-term (hourly) and medium-term (daily) forecasting.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
